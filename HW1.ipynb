{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c29809c",
   "metadata": {},
   "source": [
    "# Homework 1 - Brian Davidson\n",
    "\n",
    "To test the functions for problems 1 and 2, under the kernel menu select \"Restart & Run All\".  Then scroll to the bottom of this file to the section labeled \"Try it out\".  \n",
    "\n",
    "To generate all of the figures used in my report, set `run_everything=True` in the following cell.  Warning, it takes a little over 1 hour to run everything on a MacBook Pro with M1 processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f112d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_everything=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deec7f9",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "(35 points) A weighted coin with probability *p* of coming up heads is flipped repeatedly. You have some whole dollar amount of money that is less than a hundred dollars. At each time, you can choose to bet any whole dollar amount that is less than or equal to the amount you possess. If the coin comes up heads you double the amount you bet, and if it comes up tails you lose the entire amount you bet. Your goal is to accumulate \\\\$100  after that you retire. So there are two eventual outcomes â€“ accumulate \\\\$100 (you win), or go bust (end up with \\\\$0 and lose).\n",
    "\n",
    "The state is your capital $s \\in \\{0, 1, 2, . . . , 99, 100\\}$. The actions are the amounts you may choose to bet. In state $s, a \\in \\{1, 2, . . . , s \\}$. The states 0 and 100 represent terminal states, with rewards 0 and 1 respectively. The utility function is undiscounted, and non-terminal states have 0 utility.\n",
    "\n",
    "You will solve the problem of how much to bet at any state using value iteration. Before you start, though, think about and answer one question: what does the value function for a state represent? Please make sure you know the answer to this question before you proceed!\n",
    "Implement value iteration and solve for the optimal policy for $p = 0.25$, $p = 0.4$, and $p = 0.55$. Present your results for each case as two graphs, one showing the final value estimates as a function of the state and one showing the optimal policy (how much you should bet as a function of the state). Write down your interpretation of the forms of the optimal policies in these cases and explain the differences. You may want to check your tie-breaking policies if you see unusual behavior that is hard to explain in the context of the rest of the policy (although, fair warning, some of the policies themselves may look surprising initially!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756e1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "random.seed(687)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3150a",
   "metadata": {},
   "source": [
    "This is just testing my intuition prior to implementing value iteration...\n",
    "\n",
    "The expected value of a single bet is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(bet) & = & p\\cdot x + (1-p)\\cdot(-x)\\\\\n",
    " & = & p\\cdot x - x + p\\cdot x\\\\\n",
    " & = & 2\\cdot p\\cdot x - x\\\\\n",
    " & = & (2\\cdot p - 1)x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $p < \\frac{1}{2}$ we expect to lose money.  So we expect to lose more often than we win for both the $p = 0.25$ case and $p = 0.4$ case.\n",
    "\n",
    "My guess is that we want to bet the maximum amount for $p=0.25$ and $p=0.4$, and bet $\\$1$ at a time when $p>0.5$.  To win in the first two cases we need to get lucky.  We're far more likely to get lucky 1 time than we are to get lucky 2 or 3 times in a row.  Betting as few times as possible should maximize our chances of winning.\n",
    "\n",
    "When $p>0.5$, we don't want to risk a lot on a single bet.  It's better to make a lot of small bets, as we will win most of them (most of the time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36546ac4",
   "metadata": {},
   "source": [
    "## Testing my initial hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cd70e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgv0lEQVR4nO3dd3hUZd7G8e+TRugtAemh92rokFgIvSg2bFjonWSrq7vrrq6u6xqagIBipygCoiAQXExIqKH33mvovT/vH4n7urtgBpnJmXJ/risXE2aYuR8Sbk7OnPM7xlqLiIh4ryCnA4iIyM9TUYuIeDkVtYiIl1NRi4h4ORW1iIiXC/HEk0ZERNioqChPPLWIiF9auXLlcWtt5K3u80hRR0VFkZ6e7omnFhHxS8aYvbe7T7s+RES8nIpaRMTLqahFRLycilpExMupqEVEvJxLR30YY/YA54AbwHVrbbQnQ4mIyP+7k8Pz7rfWHvdYEhERuSWv2vUx8vvtrN1/2ukYIiJexdWitsB8Y8xKY0zvWz3AGNPbGJNujEnPyMi44yCnL15l0rJ9PDwmjTfmbObS1Rt3/BwiIv7IuHLhAGNMKWvtQWNMMSAJGGStTbnd46Ojo+0vOTPx7OVrvDlnC5OX7yOqaB7e7FqHphWL3vHziIj4GmPMytu9/+fSFrW19mDWr8eAGUAj98X7fwXCQ3mza20m9WqMBZ6csJQ/zFjP2cvXPPFyIiI+IduiNsbkNcbk//E20BrY4MlQzSpGMHdIDL1almfK8n20Tkzh+81HPfmSIiJey5Ut6uJAqjFmLbAcmG2tnevZWJA7LJiXO9Rgev/mFMwdSo+P0xk8eTUnzl/x9EuLiHgVl/ZR36lfuo/6dq5ev8mYH3YweuEO8oeH8udONehctyTGGLe9hoiIk+56H7XTwkKCGNqqCt8OakmZInkYMmUNPT9O5/CZS05HExHxOJ8o6h9VvSc/0/s145UO1UnbeZzWiSlMWraPmzfd/1OBiIi38KmiBggOMvRsWYF5Q2OoVaogf5ixnqfeX8qe4xecjiYi4hE+V9Q/Klc0L5N6NebvXWuz8eBZ2gxPYXzKTq7fuOl0NBERt/LZogYwxtCtUVmSEmJpWTmCN+Zs4ZGxi9ly5KzT0URE3Mani/pH9xQMZ0L3aEY9WZ8Dpy7RcWQqiUnbuHJdp6GLiO/zi6KGzK3rTnVLkpQQS8c6JRj5/XY6jUpl9b5TTkcTEbkrflPUPyqSN4zh3eoz8flozl2+Ttexi3nt201cvHrd6WgiIr+I3xX1jx6oVpz58TE83bgsH6Tups3wFNJ2aJy2iPgevy1qgPzhobz+UG2m9G5CsDE8/f4yfv/VOs5c0pAnEfEdfl3UP2pSoShzh8bQJ7YCX6TvJy4xmfkbjzgdS0TEJQFR1ADhocG81K46Mwc0p0jeMHp/upKBk1ZxXEOeRMTLBUxR/6hO6ULMGtiCX8VVYf7Go7RKTGbG6gN4YjiViIg7BFxRQ+aQp0EPVmb24BaUj8hL/NS1vPjRCg6d1pAnEfE+AVnUP6pcPD/T+jbjTx1rsHTXSeISk/l06V4NeRIRrxLQRQ2ZQ55ebFGe+fEx1C9bmD/O3EC38UvZlXHe6WgiIoCK+t/KFMnDpz0a8Y9H6rD5yFnajVjEe8ka8iQizlNR/4QxhscblmFBQiyxVSL5+3dbeGhMGpsOaciTiDhHRX0LxQuEM+7ZexnzdAOOnLlM53dTeWf+Vg15EhFHqKhvwxhD+9olSIqPpXO9koz61w46jExl5d6TTkcTkQCjos5G4bxhJD5ej49eaMilqzd49L0lvDprIxeuaMiTiOQMFbWL7qtajHnxMTzbpBwfLd5Dm+EpLNqe4XQsEQkAKuo7kC9XCH/tUosv+jQlLDiIZz9Yzm++XMuZixryJCKeo6L+BRqVL8KcIS3pf19Fpq8+SKthyczdoCFPIuIZKupfKDw0mN+2rcbXA5oTmS8XfT9bSf/PV3Ls3GWno4mIn1FR36VapQry9cDm/KZNVRZsPkZcYgrTVmrIk4i4j4raDUKDgxhwfyXmDG5JpWL5+PWXa3nuwxUcOHXR6Wgi4gdU1G5UqVg+vuzTlL90rkn6npO0HpbCx4v3aMiTiNwVFbWbBQUZnmsWxfz4GKKjivDnWRt5fNwSdmrIk4j8QipqDyldOA8fv9CQfz5Wl+3HztNuxCJGL9zBNQ15EpE7pKL2IGMMj95bmqSEGFpVL8bb87bS5d00Nhw843Q0EfEhLhe1MSbYGLPaGPOtJwP5o2L5wxnz9L2890wDjp27QpfRabw1dwuXr2nIk4hk7062qIcAmz0VJBC0rVWC7xNi6Vq/FGN/2En7EYtYsUdDnkTk57lU1MaY0kAH4H3PxvF/BfOE8vZjdfnkxUZcuX6Tx95bwp++3sB5DXkSkdtwdYt6OPBb4LbvhBljehtj0o0x6RkZGlaUnZgqkcyPj+H5ZlF8unQvbYalkLxNf28i8r+yLWpjTEfgmLV25c89zlo73lobba2NjoyMdFtAf5Y3Vwivdq7JtL5NCQ8N4rmJy0n4Yg2nL151OpqIeBFXtqibA52NMXuAKcADxpjPPJoqwNxbrgizB7dk4P2VmLXmEK0Sk5mz/rBOQxcRwIWitta+ZK0tba2NAroB/7LWPuPxZAEmPDSYX7epytcDm3NPwXD6f76Kvp+t5NhZDXkSCXQ6jtrL1CxZkJn9m/O7ttVYuDWDVonJfJG+X1vXIgHsjoraWvuDtbajp8JIppDgIPrdV5G5Q1pS7Z4C/HbaOp79YDn7T2rIk0gg0ha1F6sQmY8pvZvw2kO1WL3vFK2HpfBh2m5uaMiTSEBRUXu5oCDDs03KMT8hlsYVivCXbzbx2HuL2XHsnNPRRCSHqKh9RKlCufnw+YYMe6Iuu45foP2IVEZ9v11DnkQCgIrahxhjeLh+aRYkxBJXszjvJG2j06hU1h/QkCcRf6ai9kER+XIx+qkGjHv2Xk5euEqX0am8+d1mDXkS8VMqah/WpuY9JCXE8nh0GcYl76LdiEUs23XC6Vgi4mYqah9XMHcof3+kDp/3bMz1mzd5YvxSXpm5nnOXrzkdTUTcREXtJ5pXimDe0Bh6tCjP58v20WZYCgu3HHM6loi4gYraj+QJC+GPHWvwVb9m5M0VwgsfrSB+6hpOXtCQJxFfpqL2Qw3KFubbwS0Y/GBlvll7iLjEZL5Ze0inoYv4KBW1n8oVEkxCXBW+GdSCUoVzM2jyanp9spKjGvIk4nNU1H6ueokCTO/XjD+0r8ai7ZlDnqYs36etaxEfoqIOACHBQfSOqci8oTHUKFGA309fz9PvL2PfCQ15EvEFKuoAEhWRl8m9mvDGw7VZd+AMrYcn8/6iXRryJOLlVNQBJijI8FTjsiQlxNCsYgSvz95M17GL2XpEQ55EvJWKOkCVKJibD56LZkS3euw/eZGOoxYxfME2rl7XkCcRb6OiDmDGGLrUK0VSfAzta5dg+ILtdBqVytr9p52OJiI/oaIWiubLxYhu9Xm/ezRnLl3j4TFp/G32Ji5d1ZAnEW+gopZ/a1WjOPMTYujWqCwTFu2m7YgUluzUkCcRp6mo5T8UCA/ljYdrM6lXYwCenLCUl6av56yGPIk4RkUtt9SsYgRzh8TQO6YCU1fsIy4xmQWbjjodSyQgqajltnKHBfOH9tWZ3r85hXKH0fOTdAZPXs2J81ecjiYSUFTUkq16ZQrxzaAWxLeqwncbDtMqMZmv1xzUaegiOURFLS4JCwliSKvKzB7cknJF8zJkyhp6fpzO4TOXnI4m4vdU1HJHqhTPz1f9mvFKh+qk7TxOXGIKny/by02dhi7iMSpquWPBQYaeLSswf2gsdUoX5OUZG3jq/aXsOX7B6WgifklFLb9Y2aJ5+LxnY/7etTYbD56lzfAUxqfs5PoNnYYu4k4qarkrxhi6NSpLUkIsLStH8sacLXQdu5jNh886HU3Eb6ioxS3uKRjOhO738u5T9Tl46hKdRqWSmLSNK9d1GrrI3VJRi9sYY+hYpyQLEmLpVLckI7/fTseRqazad8rpaCI+LduiNsaEG2OWG2PWGmM2GmP+khPBxHcVzhvGsCfq8eHzDTl/5TqPjF3Ma99u4uLV605HE/FJrmxRXwEesNbWBeoBbY0xTTyaSvzC/dWKMT8+hqcbl+WD1N20GZ5C2o7jTscS8TnZFrXNdD7r09CsDx00Ky7JHx7K6w/VZmrvJoQEBfH0+8v43bR1nLmkIU8irnJpH7UxJtgYswY4BiRZa5fd4jG9jTHpxpj0jIwMN8cUX9e4QlG+G9KSvrEVmbbqAHGJyczfeMTpWCI+waWittbesNbWA0oDjYwxtW7xmPHW2mhrbXRkZKSbY4o/CA8N5vftqjGzf3OK5stF709XMmDSKjLOaciTyM+5o6M+rLWngYVAW4+kkYBQu3RBZg1szq9bVyFp41HihiUzY/UBDXkSuQ1XjvqINMYUyrqdG4gDtng4l/i50OAgBj5QmTlDWlAhIi/xU9fywkcrOHhaQ55E/psrW9QlgIXGmHXACjL3UX/r2VgSKCoVy8+XfZvx5041WLbrJK0Tk/l0yR4NeRL5CeOJHzejo6Ntenq6259X/Nv+kxf5w4z1LNp+nEZRRfj7I7WpEJnP6VgiOcIYs9JaG32r+3RmoniNMkXy8MmLjXj70TpsOXKWtiMWMfYHDXkSUVGLVzHG8Fh0GRYkxHJ/1UjemruFh8aksemQhjxJ4FJRi1cqViCccc9GM/bpBhw5c4XO76byz3lbuXxNQ54k8Kioxau1q12CBQkxdKlXincX7qDDyEWs3HvS6VgiOUpFLV6vUJ4w3nm8Lh+/2IjL127y6HtLeHXWRi5c0ZAnCQwqavEZsVUimRcfQ/cm5fh4yR5aD0shZZvGFYj/U1GLT8mXK4S/dKnFF32akis0iO4Tl/PrL9dy5qKGPIn/UlGLT2oYVYQ5g1vS/76KzFh9kFbDkpm74bDTsUQ8QkUtPis8NJjftq3G1wOaE5kvF30/W0W/z1Zy7Nxlp6OJuJWKWnxerVIF+Xpgc37TpirfbzlGXGIK01ZqyJP4DxW1+IXQ4CAG3F+JOYNbUrlYPn795Vq6T1zO/pMXnY4mctdU1OJXKhXLxxd9mvLXLjVZtfcUbYan8FHabg15Ep+moha/ExRk6N40innxMURHFeHVbzbx+Lgl7Dh2Pvs/LOKFVNTit0oXzsPHLzTkncfqsv3YedqPWMTohTu4piFP4mNU1OLXjDE8cm9pFiTE0qpGMd6et5Uu76ax4eAZp6OJuExFLQEhMn8uxjx9L+8904CM81foMjqNt+Zu0ZAn8QkqagkobWuVYEF8LI80KMXYH3bSfsQiVuzRkCfxbipqCTgF84Tyj0fr8lmPxly9cZPH3lvCn77ewHkNeRIvpaKWgNWicgTzhsbwQvMoPl26lzbDUvhh6zGnY4n8DxW1BLS8uUL4c6eaTOvbjNxhwTz/4QoSvljDqQtXnY4m8m8qahHg3nKFmT24BYMeqMSsNYeIG5bM7HWHdRq6eAUVtUiWXCHB/Kp1VWYNbEGJgrkZMGkVfT5dybGzGvIkzlJRi/yXGiULMKN/M15qV43kbRk8mJjMFyv2a+taHKOiFrmFkOAg+sRW5LshLaleogC//Wodz36gIU/iDBW1yM+oEJmPKb2a8PpDtViz/zSth6UwMXU3NzTkSXKQilokG0FBhmealGN+fAyNKxThr99u4rH3FrP96Dmno0mAUFGLuKhkodx8+HxDhj9Rj93HL9BhZCqjvt/O1esa8iSepaIWuQPGGB6qX4qkhFja1LqHd5K20fndVNYdOO10NPFjKmqRXyAiXy5GPVmfCd2jOXXxKg+NTuPNOZs15Ek8QkUtchfiahRnfnwsTzQsw7iUXbQdnsLSXSecjiV+RkUtcpcK5g7lza51mNSzMTctdBu/lJdnrOfc5WtORxM/kW1RG2PKGGMWGmM2GWM2GmOG5EQwEV/TrFIEc4e2pGeL8kxevo/Ww1JYuEVDnuTuubJFfR34lbW2BtAEGGCMqeHZWCK+KU9YCK90rMFX/ZqRL1cIL3y0gqFTVnNSQ57kLmRb1Nbaw9baVVm3zwGbgVKeDibiy+qXLcy3g1sw5MHKzF5/mFaJycxae0inocsvckf7qI0xUUB9YNkt7uttjEk3xqRnZGS4KZ6I78oVEkx8XBW+GdSCMoVzM3jyanp9spIjZzTkSe6McfV/eGNMPiAZ+Ju1dvrPPTY6Otqmp6e7IZ6If7hx0zIxdTfvJG0lNCiIP3SoTreGZTDGOB1NvIQxZqW1NvpW97m0RW2MCQW+Aj7PrqRF5H8FBxl6xVRg7pAYapYqwEvT1/PUhGXsPXHB6WjiA1w56sMAHwCbrbWJno8k4r+iIvIyqWcT3ni4NhsOnqHN8BTeX7RLQ57kZ7myRd0ceBZ4wBizJuujvYdzifitoCDDU43LMj8hhuYVI3h99ma6jl3M1iMa8iS35vI+6juhfdQirrHW8s26w7w6ayPnLl9jwP2V6H9fJcJCdC5aoLnrfdQi4hnGGDrXLcmChFja1y7B8AXb6TQqlTX7TzsdTbyIilrECxTJG8aIbvX54Llozly6Rtcxafxt9iYuXdWQJ1FRi3iVB6sXZ35CDN0alWXCot20GZ7C4p3HnY4lDlNRi3iZAuGhvPFwbSb3aoIx8NSEZbw0fT1nNeQpYKmoRbxU04pFmTskhj4xFZi6Yh9xicks2HTU6VjiABW1iBfLHRbMS+2rM3NAcwrnCaPnJ+kMmryaE+evOB1NcpCKWsQH1CldiFkDW5AQV4W5GzKHPH295qCGPAUIFbWIjwgLCWLwg5WZPbgl5YrmZciUNfT4OJ1Dpy85HU08TEUt4mOqFM/PV/2a8ceONViy8wSth6Xw+bK93NRp6H5LRS3ig4KDDD1alGfe0BjqlinIyzM28OSEpew+riFP/khFLeLDyhbNw2c9GvPWI7XZdPgsbYenMC55J9dv3HQ6mriRilrExxljeKJhWRYkxBJTJZI3v9tC17GL2Xz4rNPRxE1U1CJ+oniBcMY/ey+jn2rAodOX6DQqlcT5W7lyXaeh+zoVtYgfMcbQoU4JkuJj6Vy3JCP/tYOOI1NZte+U09HkLqioRfxQ4bxhJD5Rjw9faMiFK9d5ZOxi/vrNJi5eve50NPkFVNQifuz+qsWYFx/DM43LMTEtc8hT6nYNefI1KmoRP5c/PJTXHqrFF32aEhIUxDMfLOO309Zy5pKGPPkKFbVIgGhUvgjfDWlJv/sq8tWqg8QlJjNv4xGnY4kLVNQiASQ8NJjfta3GzP7NKZovF30+XcmAz1eRcU5DnryZilokANUuXZBZA5vzmzZVSdp0lLhhyUxfdUBDnryUilokQIUGBzHg/krMGdKCChF5SfhiLc9/uIKDGvLkdVTUIgGuUrH8fNm3Ga92qsGKPSdpnZjMJ0v2aMiTF1FRiwjBQYbnm2cOeWpQrjB/+nojT4xfws6M805HE1TUIvITZYrk4ZMXG/H2o3XYeuQc7UYsYswPOzTkyWEqahH5D8YYHosuw4JfxfJA1WL8Y+5WHhqTxsZDZ5yOFrBU1CJyS8Xyh/Pes/cy9ukGHDlzhc7vpvH2vC1cvqYhTzlNRS0iP6td7RIsSIjh4fqlGL1wJx1GLiJ9z0mnYwUUFbWIZKtQnjD++VhdPnmxEZev3eSxcUt4ddZGLlzRkKecoKIWEZfFVIlkfnwMzzWN4uMle2g9LIWUbRlOx/J7KmoRuSN5c4XwaueafNmnKblCg+g+cTm//nItpy9edTqa38q2qI0xE40xx4wxG3IikIj4huioIswZ3JIB91dkxuqDtEpM4bv1h52O5Zdc2aL+CGjr4Rwi4oPCQ4P5TZtqzBrYnOIFctHv81X0+2wlx85ddjqaX8m2qK21KYDe4hWR26pZsiAzBzTnd22r8f2WY8QlpvBl+n4NeXITt+2jNsb0NsakG2PSMzL05oJIoAkNDqLffRX5bkhLqhTPx2+mraP7xOXsP3nR6Wg+z21Fba0db62NttZGR0ZGuutpRcTHVIzMx9TeTXmtS01W7T1Fm+EpfJS2W0Oe7oKO+hARtwsKMjzbNIp58TE0jCrCq99s4rFxS9hx7JzT0XySilpEPKZ04Tx89EJDEh+vy86M87QfkcrohTu4piFPd8SVw/MmA0uAqsaYA8aYHp6PJSL+whhD1walSYqPJa5mcd6et5Uu76ax4aCGPLnKeOJd2ejoaJuenu725xUR3zdv4xFembmBkxeu0jumAkMerEx4aLDTsRxnjFlprY2+1X3a9SEiOapNzXtYEB/Low1KM/aHnbQfsYjlu3UE8M9RUYtIjiuYJ5S3Hq3DZz0ac/XGTR4ft4Q/ztzAeQ15uiUVtYg4pkXlCObHx/Bi8/J8tmwvrROTWbj1mNOxvI6KWkQclScshD91qsG0vs3IkyuEFz5cQcLUNZy6oCFPP1JRi4hXuLdcYWYPbsHgByoxa+0h4oYlM3vdYZ2GjopaRLxIrpBgElpX5ZtBLShRMDcDJq2iz6crOXo2sIc8qahFxOtUL1GAGf2b8VK7aiRvy6BVYjJTV+wL2K1rFbWIeKWQ4CD6xFZk7tAYqpcowO++Ws8zHyxj34nAG/KkohYRr1Y+Ii9TejXh9YdqsXb/GdoMT+GD1N3cCKAhTypqEfF6QUGGZ5qUY358DE0qFOG1bzfx6HuL2X40MIY8qahFxGeULJSbic83ZES3euw5foEOI1MZ+f12rl737yFPKmoR8SnGGLrUK8WChFja1LqHxKRtdH43lbX7TzsdzWNU1CLik4rmy8WoJ+szoXs0py5e5eExabw5ZzOXrt5wOprbqahFxKfF1ShOUkIsTzQsw7iUXbQbkcLSXSecjuVWKmoR8XkFwkN5s2sdJvVszE0L3cYv5eUZ6zl3+ZrT0dxCRS0ifqNZpQjmDY2hV8vyTF6+j9bDUvjXlqNOx7prKmoR8Su5w4J5uUMNpvdvToHwUF78KJ0hU1Zz4vwVp6P9YipqEfFL9coU4ptBLRjaqjJz1h8mblgKs9Ye8snT0FXUIuK3wkKCGNqqCt8OakmZInkYPHk1vT5J58gZ3xrypKIWEb9X9Z78TO/XjFc6VCd1x3HiEpOZvNx3hjypqEUkIAQHGXq2rMC8oTHUKlWQl6av56kJy9h74oLT0bKlohaRgFKuaF4m9WrMm11rs+Fg5pCnCSm7vHrIk4paRAKOMYYnG5UlKSGWFpUi+NuczXQdk8bWI9455ElFLSIB656C4UzoHs2oJ+tz4NQlOo5axLCkbV435ElFLSIBzRhDp7olSUqIpUPtEoz4fjsdRy1ijRcNeVJRi4gARfKGMbxbfSY+H825y9fpOiaN17/d5BVDnlTUIiI/8UC14syPj+HJRmV5P3U3bYansHjncUczqahFRP5L/vBQ/vZwbab0bkKQgacmLOOl6es4c8mZIU8qahGR22hSoShzh8bQJ7YCU1fsp/WwZJI25fyQJxW1iMjPCA8N5qV21Zk5oDmF84TR65N0Bk5axfEcHPKkohYRcUGd0oWYNbAFv4qrwvyNR4lLTGbm6oM5chq6S0VtjGlrjNlqjNlhjPm9p0OJiHijsJAgBj1YmdmDWxAVkZehU9fQ4+N0Dp2+5NHXzbaojTHBwGigHVADeNIYU8OjqUREvFjl4vmZ1rcZf+pYgyU7T9B6WAqfLd3LTQ+dhu7KFnUjYIe1dpe19iowBejikTQiIj4iOMjwYovyzI+PoV6ZQrwycwPdJizl4tXrbn8tV4q6FLD/J58fyPq9/2CM6W2MSTfGpGdkZLgrn4iIVytTJA+f9mjEPx6pQ/mieckTFuL213Dbm4nW2vHW2mhrbXRkZKS7nlZExOsZY3i8YRneerSOR57flaI+CJT5yeels35PRERygCtFvQKobIwpb4wJA7oBszwbS0REfpTtzhRr7XVjzEBgHhAMTLTWbvR4MhERAVwoagBr7RxgjoeziIjILejMRBERL6eiFhHxcipqEREvp6IWEfFyxhOTn4wxGcDeX/jHIwBnL6eQ87Rm/xdo6wWt+U6Vs9be8mxBjxT13TDGpFtro53OkZO0Zv8XaOsFrdmdtOtDRMTLqahFRLycNxb1eKcDOEBr9n+Btl7Qmt3G6/ZRi4jIf/LGLWoREfkJFbWIiJdzrKizu2CuMSaXMWZq1v3LjDFRDsR0GxfWm2CM2WSMWWeM+d4YU86JnO7k6kWRjTGPGGOsMcbnD+VyZc3GmMezvtYbjTGTcjqju7nwvV3WGLPQGLM66/u7vRM53cUYM9EYc8wYs+E29xtjzMisv491xpgGd/2i1toc/yBzXOpOoAIQBqwFavzXY/oD72Xd7gZMdSJrDq73fiBP1u1+vrxeV9ec9bj8QAqwFIh2OncOfJ0rA6uBwlmfF3M6dw6seTzQL+t2DWCP07nvcs0xQANgw23ubw98BxigCbDsbl/TqS1qVy6Y2wX4OOv2NOBBY4zJwYzulO16rbULrbUXsz5dSuaVdHyZqxdFfg14C7ick+E8xJU19wJGW2tPAVhrj+VwRndzZc0WKJB1uyBwKAfzuZ21NgU4+TMP6QJ8YjMtBQoZY0rczWs6VdSuXDD334+x1l4HzgBFcySd+7l0geCf6EHm/8i+LNs1Z/1IWMZaOzsng3mQK1/nKkAVY0yaMWapMaZtjqXzDFfW/CrwjDHmAJlz7QflTDTH3Om/92y5/3K5cleMMc8A0UCs01k8yRgTBCQCzzscJaeFkLn74z4yf2pKMcbUttaedjKUhz0JfGStfccY0xT41BhTy1p70+lgvsKpLWpXLpj778cYY0LI/JHpRI6kcz+XLhBsjGkFvAx0ttZeyaFsnpLdmvMDtYAfjDF7yNyXN8vH31B05et8AJhlrb1mrd0NbCOzuH2VK2vuAXwBYK1dAoSTObzIX7n9guBOFbUrF8ydBTyXdftR4F82a0+9D8p2vcaY+sA4Mkva1/dbQjZrttaesdZGWGujrLVRZO6X72ytTXcmrlu48n09k8ytaYwxEWTuCtmVgxndzZU17wMeBDDGVCezqDNyNGXOmgV0zzr6owlwxlp7+K6e0cF3TtuTuTWxE3g56/f+SuY/Vsj8Yn4J7ACWAxWcfrfXw+tdABwF1mR9zHI6s6fX/F+P/QEfP+rDxa+zIXOXzyZgPdDN6cw5sOYaQBqZR4SsAVo7nfku1zsZOAxcI/MnpB5AX6DvT77Go7P+Pta74/tap5CLiHg5nZkoIuLlVNQiIl5ORS0i4uVU1CIiXk5FLSLi5VTUIiJeTkUtIuLl/g+q+rnd3XDlAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.2\n"
     ]
    }
   ],
   "source": [
    "class Gambler(object):\n",
    "    def __init__(self, cash = None, probability=0.25):\n",
    "        self.probability = probability\n",
    "        if cash is None:\n",
    "            self.cash = random.randint(1,99)\n",
    "        else:\n",
    "            self.cash = cash\n",
    "    def bet(self, wager):\n",
    "        if random.random() < self.probability:\n",
    "            self.cash += wager\n",
    "        else:\n",
    "            self.cash -= wager\n",
    "    def get_cash(self):\n",
    "        return self.cash\n",
    "    def get_actions(self):\n",
    "        top = min(100-self.cash, self.cash)\n",
    "        actions = [*range(1,top+1,1)]\n",
    "        return(actions)\n",
    "    \n",
    "gambler = Gambler(probability=.25)\n",
    "mycash = []\n",
    "mycash.append(gambler.get_cash())\n",
    "#print(f\"{gambler.get_cash()} {gambler.get_actions()}\")\n",
    "\n",
    "sum25 = []\n",
    "for ii in range(1000):\n",
    "    gambler = Gambler(probability=.25)\n",
    "\n",
    "    mycash = []\n",
    "    mycash.append(gambler.get_cash())\n",
    "    while gambler.get_cash() < 100 and gambler.get_cash() > 0:    \n",
    "        gambler.bet(max(gambler.get_actions()))\n",
    "        mycash.append(gambler.get_cash())\n",
    "        #print(f\"{gambler.get_cash()} {gambler.get_actions()}\")\n",
    "    sum25.append(gambler.get_cash())\n",
    "plt.plot(mycash)\n",
    "plt.show()\n",
    "#print(sum25)\n",
    "print(sum(sum25)/len(sum25))\n",
    "\n",
    "sum40 = []\n",
    "for ii in range(1000):\n",
    "    gambler = Gambler(probability=.4)\n",
    "    mycash = []\n",
    "    mycash.append(gambler.get_cash())\n",
    "    while gambler.get_cash() < 100 and gambler.get_cash() > 0:    \n",
    "        gambler.bet(max(gambler.get_actions()))\n",
    "        mycash.append(gambler.get_cash())\n",
    "    sum40.append(gambler.get_cash())\n",
    "\n",
    "plt.plot(mycash)\n",
    "plt.show()\n",
    "#print(sum40)\n",
    "print(sum(sum40)/len(sum40))\n",
    "\n",
    "sum55 = []\n",
    "for ii in range(1000):\n",
    "    gambler = Gambler(probability=.55)\n",
    "    mycash = []\n",
    "    mycash.append(gambler.get_cash())\n",
    "    while gambler.get_cash() < 100 and gambler.get_cash() > 0:    \n",
    "        gambler.bet(1)\n",
    "        mycash.append(gambler.get_cash())\n",
    "    sum55.append(gambler.get_cash())\n",
    "plt.plot(mycash)\n",
    "plt.show()\n",
    "#print(sum55)\n",
    "print(sum(sum55)/len(sum55))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472196e7",
   "metadata": {},
   "source": [
    "$$\n",
    "R(s) = \\begin{cases}\n",
    "1 & \\text{when } s=100\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma = 1.0\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_{i+1}(s) = R(s) + \\gamma \\max_{a\\in A(s)} \\sum_{s'}P(s'|s,a)U_i(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ba564c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def actions(state):\n",
    "    top = min(100-state, state)\n",
    "    actions = [*range(1,top+1,1)]\n",
    "    return(actions)\n",
    "\n",
    "def R(state):\n",
    "    r=0\n",
    "    if state == 100:\n",
    "        r =100\n",
    "    return r\n",
    "\n",
    "\n",
    "def value_iteration(p):\n",
    "    U = np.random.random_sample(101)\n",
    "    #U = np.ones(101)\n",
    "    #U = np.zeros(101)\n",
    "    U[100] = 1\n",
    "    U[0] = 0\n",
    "    all_values = []\n",
    "    all_values.append(U.copy())\n",
    "    total_iterations = 0\n",
    "    for ii in range(100000):\n",
    "        U_new = np.zeros(101)\n",
    "        U_new[100] = 1\n",
    "        for s in [*range(1,100,1)]:\n",
    "            r = R(s)\n",
    "        \n",
    "            best_a = 0\n",
    "            best_val = -999999.0\n",
    "            for a in actions(s):\n",
    "                val = p*U[s+a] + (1-p)*U[s-a]\n",
    "                if val > best_val:\n",
    "                    best_a = a\n",
    "                    best_val = val\n",
    "            U_new[s] = r + best_val\n",
    "        if np.array_equal(U, U_new):\n",
    "            print(f\"converged after {ii} iterations\")\n",
    "            total_iterations=ii\n",
    "            break\n",
    "        U = U_new\n",
    "        all_values.append(U.copy())\n",
    "    return U, all_values, total_iterations\n",
    "#print(U)\n",
    "\n",
    "\n",
    "def compute_policy(U, p, debug=[]):\n",
    "    pol = np.zeros((101,), dtype=int)\n",
    "\n",
    "    if (len(debug) > 0):\n",
    "        print(f\"{U}\")\n",
    "    for s in [*range(1,100,1)]:\n",
    "        best_a = 0\n",
    "        best_val = -999999.0\n",
    "        for a in actions(s):\n",
    "            val = p*U[s+a] + (1-p)*U[s-a]\n",
    "            if (s in debug):\n",
    "                print(f\"action {a}: {p}*{U[s+a]} + (1-{p})*{U[s-a]} = {val}\")\n",
    "            if val >= best_val:\n",
    "                best_a = a\n",
    "                best_val = val\n",
    "        pol[s] = best_a\n",
    "        if (s in debug):\n",
    "            print(f\"pol[{s}] = {best_a}\")\n",
    "    return pol\n",
    "\n",
    "def gamble(pol, p, U, iterations=1000):\n",
    "    winning = []\n",
    "    for starting_cash in range(100):\n",
    "        sum40 = []\n",
    "        for ii in range(iterations):\n",
    "    \n",
    "            gambler = Gambler(cash = starting_cash, probability=p)\n",
    "            mycash = []\n",
    "            mycash.append(gambler.get_cash())\n",
    "            while gambler.get_cash() < 100 and gambler.get_cash() > 0:    \n",
    "                gambler.bet(pol[gambler.get_cash()])\n",
    "                mycash.append(gambler.get_cash())\n",
    "            sum40.append(gambler.get_cash())\n",
    "\n",
    "        print(f\"{starting_cash}\\t{sum(sum40)/len(sum40)} {U[starting_cash]*100}\")\n",
    "\n",
    "        winning.append(sum(sum40)/len(sum40))\n",
    "    return winning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1ec35c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_everything:\n",
    "    p = .25\n",
    "    U, all_values, num_iterations = value_iteration(p)\n",
    "    pol = compute_policy(U, p)\n",
    "\n",
    "    print(pol)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Iteration #\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values, aspect='auto', cmap=mpl.colormaps['Spectral'])\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('AllValues_25.pdf', format='pdf')\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(U)\n",
    "    ax.set_ylabel(\"U*(s) (value of being in state)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    plt.savefig('Value_25.pdf', format='pdf')\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylim(0,50)\n",
    "    ax.plot(pol, 'b*')\n",
    "    ax.set_ylabel(\"action ($ to bet)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    plt.savefig('Policy_25.pdf', format='pdf')\n",
    "\n",
    "\n",
    "    p = .4\n",
    "    U, all_values, num_iterations = value_iteration(p)\n",
    "    pol = compute_policy(U,p)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Iteration #\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values, aspect='auto', cmap=mpl.colormaps['Spectral'])\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('AllValues_4.pdf', format='pdf')\n",
    "\n",
    "\n",
    "    print(pol)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(U)\n",
    "    ax.set_ylabel(\"U*(s) (value of being in state)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    plt.savefig('Value_4.pdf', format='pdf')\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylim(0,50)\n",
    "    ax.plot(pol)\n",
    "    ax.set_ylabel(\"action ($ to bet)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    plt.savefig('Policy_4.pdf', format='pdf')\n",
    "\n",
    "\n",
    "    p = .55\n",
    "    U, all_values, num_iterations = value_iteration(p)\n",
    "    pol = compute_policy(U,p)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Iteration #\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values, aspect='auto', cmap=mpl.colormaps['Spectral'])\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('AllValues_55.pdf', format='pdf')\n",
    "\n",
    "    print(U)\n",
    "    print(pol)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(U)\n",
    "    ax.set_ylabel(\"U*(s) (value of being in state)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    plt.savefig('Value_55.pdf', format='pdf')\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylim(0,50)\n",
    "    ax.plot(pol, 'b*')\n",
    "    ax.set_ylabel(\"action ($ to bet)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    plt.savefig('Policy_55.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe73a92",
   "metadata": {},
   "source": [
    "## Investigating why so many iterations for p=.55\n",
    "\n",
    "The idea is to run value iteration on all values of $p$ from $.01$ to $1.0$ in increments of $.01$ and take a look at the graph.  My hypothesis is that it takes many more iterations for values of $p$ that are close to $.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fe194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_everything:\n",
    "    iterationlist = []\n",
    "    for testp in range(100):\n",
    "        tmp, ignore, num_iterations = value_iteration((testp+1)/100)\n",
    "        print(f\"{(testp+1)/100} {num_iterations}\")\n",
    "        iterationlist.append(((testp+1)/100,num_iterations))\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    plt.plot(*zip(*iterationlist))\n",
    "    ax.set_ylabel(\"# iterations\")\n",
    "    ax.set_xlabel(\"p\")\n",
    "    plt.savefig('ptoiterations.pdf', format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02563459",
   "metadata": {},
   "source": [
    "## problem 2\n",
    "(35 points) Implement three different algorithms for multi-armed Bernoulli bandits: the greedy algorithm, UCB1, and Thompson Sampling. You should write a function for each of the three, which takes as input (1) a vector containing the probabilities with which each arm gives a payoff of 1, and (2) the number of iterations to run for. Test your algorithms on two different settings: (1) an eleven-armed bandit with payoff probabilities $0,0.1,0.2,\\dots,1.0$, and (2) a five-armed bandit with payoff probabilities $0.3, 0.5, 0.7, 0.83, 0.85$.\n",
    "\n",
    "Use these test cases to analyze the empirical properties of these algorithms, at least in terms of regret over time and probability of choosing the best action over time. Minimally, you should present a graph of average regret vs. time for each of the algorithms, and some kind of visualization of which action they are choosing over time. Be sure to label your axes and caption your figures appropriately. I would suggest trying the algorithms with at least $10^3$, $10^4$, $10^5$ time steps, and also re-running them from the start many times and averaging (so, for example, run the experiment with $10^3$ time steps several hundred times). What do your results tell you about the properties of these three different algorithms?\n",
    "\n",
    "On this question in particular, I urge you to experiment and also present and write up any other interesting insights you may have (for example, playing with different probability vectors or different ways of analyzing the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5328fa4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{x}_j = \\frac{k}{k+1}\\bar{x}_j + \\frac{1}{k+1}x_j\n",
    "$$\n",
    "$$\n",
    "k = k+1\n",
    "$$\n",
    "\n",
    "where $x_j$ is the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e763b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Bandit(object):\n",
    "    def __init__(self, p=None):\n",
    "        if p is None:\n",
    "            self.p = random.uniform(0,1)\n",
    "        else:\n",
    "            self.p = p\n",
    "    def pull_arm(self):\n",
    "        if random.uniform(0,1) < self.p:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def get_probability(self):\n",
    "        return self.p\n",
    "\n",
    "class Greedy_bandits(object):\n",
    "    def __init__(self, probabilities , iterations=1000):\n",
    "        self.iterations=iterations\n",
    "        self.bandits = [Bandit(p=prob) for prob in probabilities]\n",
    "        self.p = probabilities.copy()\n",
    "        \n",
    "        #random.shuffle(self.bandits)\n",
    "        # Keep track of how many times each arm has been pulled\n",
    "        self.numPulls = np.zeros(len(probabilities), dtype=int)\n",
    "        \n",
    "        # Keep track of the rewards earned for each arm\n",
    "        self.rewards = np.zeros(len(probabilities))\n",
    "        \n",
    "        # value[arm] = rewards[arm]/numPulls[arm]\n",
    "        self.values = np.zeros(len(probabilities))\n",
    "        \n",
    "        self.not_initialized = [*range(0,len(self.bandits),1)]\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        result = 0\n",
    "        actions = []\n",
    "        regret = []\n",
    "        not_initialized = [*range(len(self.bandits))]\n",
    "        mu = 0\n",
    "        for ii in range(self.iterations):\n",
    "            if (len(not_initialized) > 0):\n",
    "                bestAction = not_initialized.pop()\n",
    "            else:\n",
    "                bestAction = np.argmax(self.values)\n",
    "            tmp = self.bandits[bestAction].pull_arm()\n",
    "            actions.append(bestAction)\n",
    "            self.rewards[bestAction] += tmp\n",
    "            result += tmp\n",
    "            self.numPulls[bestAction] += 1\n",
    "            self.values[bestAction] = self.rewards[bestAction]/self.numPulls[bestAction]\n",
    "            mu += self.p[bestAction]\n",
    "            # Regret is expected best value - expected value from chosen bandit\n",
    "            regret.append(max(self.p)*ii - mu)\n",
    "        \n",
    "        return result, actions, regret\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        #result += \"bandit\\tp\\treward\\tnumPulls\\n\"\n",
    "        total = 0\n",
    "        totalPulls = 0\n",
    "        for ii, reward in enumerate(self.rewards):\n",
    "            total += reward\n",
    "            #result += f\"{ii}\\t{self.bandits[ii].get_probability()}\\t{reward}\\t{self.numPulls[ii]}\\n\"\n",
    "            totalPulls += self.numPulls[ii]\n",
    "        result += f\"Reward {total} / Total # Pulls: {totalPulls} = {total / totalPulls}\"\n",
    "        return result\n",
    "    \n",
    "    def get_reward(self):\n",
    "        result = 0\n",
    "        for ii, reward in enumerate(self.rewards):\n",
    "            result += reward\n",
    "        return result\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7111f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_everything:\n",
    "    iterations = 1000\n",
    "    probabilities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    for i in range(iterations):\n",
    "        greedy = Greedy_bandits(probabilities = probabilities, iterations=100000)\n",
    "    \n",
    "        result, actions, regret = greedy.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "        \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('greedy-regret1.pdf', format='pdf')\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('Actions_greedy1.pdf', format='pdf')\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(results)\n",
    "    ax.set_ylabel(\"$ winnings\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    plt.savefig('greedy1.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylabel(\"# runs\")\n",
    "    ax.set_xlabel(\"$ winnings\")\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    ax.hist(results, bins='auto')\n",
    "    plt.savefig('greedy1-histo.pdf', format='pdf')\n",
    "    \n",
    "    \n",
    "    probabilities = [0.3,0.5,0.7,0.83,0.85]\n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        greedy = Greedy_bandits(probabilities = probabilities, iterations=100000)\n",
    "        result, actions, regret = greedy.run()\n",
    "        regretlist.append(regret)\n",
    "        results.append(result)\n",
    "        actionlists.append(actions)\n",
    "    \n",
    "        \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('greedy-regret2.pdf', format='pdf')\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('Actions_greedy2.pdf', format='pdf')\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(results)\n",
    "    ax.set_ylabel(\"$ winnings\")\n",
    "    ax.set_xlabel(\"run #\")\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    plt.savefig('greedy2.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylabel(\"# runs\")\n",
    "    ax.set_xlabel(\"$ winnings\")\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    ax.hist(results, bins='auto')\n",
    "    plt.savefig('greedy2-histo.pdf', format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca69e90",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound\n",
    "\n",
    "$$\n",
    "\\text{argmax}_j \\bar{x}_j + \\sqrt{\\frac{2\\ln n}{n_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b503dc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class UCB1_bandits(object):\n",
    "    def __init__(self, probabilities , iterations=1000):\n",
    "        self.iterations=iterations\n",
    "        self.bandits = [Bandit(p=prob) for prob in probabilities]\n",
    "        self.p = probabilities.copy()\n",
    "        \n",
    "        # Keep track of how many times each arm has been pulled\n",
    "        self.numPulls = np.zeros(len(probabilities), dtype=int)\n",
    "        \n",
    "        # Keep track of the rewards earned for each arm\n",
    "        self.rewards = np.zeros(len(probabilities))\n",
    "        \n",
    "        self.totalPulls = 0\n",
    "\n",
    "    def run(self):\n",
    "        result = 0\n",
    "        actions = []\n",
    "        regret = []\n",
    "        mu = 0\n",
    "        for ii in range(self.iterations):\n",
    "            bestValue = 0\n",
    "            bestAction = 0\n",
    "            for k, num_pulls in enumerate(self.numPulls):\n",
    "                if (self.numPulls[k] == 0):\n",
    "                    bestAction = k\n",
    "                    break\n",
    "                tmp_x = self.rewards[k]/self.numPulls[k] + np.sqrt(2*np.log(self.totalPulls)/self.numPulls[k])\n",
    "                if tmp_x > bestValue:\n",
    "                    bestValue = tmp_x\n",
    "                    bestAction = k\n",
    "            tmp = self.bandits[bestAction].pull_arm()\n",
    "            actions.append(bestAction)\n",
    "            self.rewards[bestAction] += tmp\n",
    "            result += tmp\n",
    "            self.numPulls[bestAction] += 1\n",
    "            self.totalPulls += 1\n",
    "            \n",
    "            mu += self.p[bestAction]\n",
    "            \n",
    "            # Regret is expected best value - expected value from chosen bandit\n",
    "            regret.append(max(self.p)*ii - mu)\n",
    "        \n",
    "        return result, actions, regret\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        #result += \"bandit\\tp\\treward\\tnumPulls\\n\"\n",
    "        total = 0\n",
    "        totalPulls = 0\n",
    "        for ii, reward in enumerate(self.rewards):\n",
    "            total += reward\n",
    "            #result += f\"{ii}\\t{self.bandits[ii].get_probability()}\\t{reward}\\t{self.numPulls[ii]}\\n\"\n",
    "            totalPulls += self.numPulls[ii]\n",
    "        result += f\"Reward {total} / Total # Pulls: {totalPulls} = {total / totalPulls}\"\n",
    "        return result\n",
    "    \n",
    "    def get_reward(self):\n",
    "        result = 0\n",
    "        for ii, reward in enumerate(self.rewards):\n",
    "            result += reward\n",
    "        return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c57d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_everything:\n",
    "    probabilities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    iterations=1000\n",
    "    \n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    for i in range(iterations):\n",
    "        ucb1 = UCB1_bandits(probabilities = probabilities, iterations=100000)\n",
    "    \n",
    "        result, actions, regret = ucb1.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('ucb-regret1.pdf', format='pdf')\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('Actions_ucb1.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(results)\n",
    "    ax.set_ylabel(\"$ winnings\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('ucb1.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylabel(\"# runs\")\n",
    "    ax.set_xlabel(\"$ winnings\")\n",
    "    ax.hist(results, bins='auto')\n",
    "    plt.savefig('ucb1-histo.pdf', format='pdf')\n",
    "    \n",
    "    \n",
    "    probabilities = [0.3,0.5,0.7,0.83,0.85]\n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        ucb1 = UCB1_bandits(probabilities = probabilities, iterations=100000)\n",
    "        result, actions, regret = ucb1.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "        \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('ucb-regret2.pdf', format='pdf')\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('Actions_ucb2.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(results)\n",
    "    ax.set_ylabel(\"$ winnings\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('ucb2.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylabel(\"# runs\")\n",
    "    ax.set_xlabel(\"$ winnings\")\n",
    "    ax.hist(results, bins='auto')\n",
    "    plt.savefig('ucb2-histo.pdf', format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e6a01",
   "metadata": {},
   "source": [
    "## Thompson Sampling\n",
    "\n",
    "Maintain a belief over ($p_1, p_2,\\dots,p_k$)\n",
    "\n",
    "At each step:\n",
    "- sample a mean reward vector based on current belief\n",
    "- choose arm with highest mean & update\n",
    "\n",
    "Use Beta function, $\\alpha$ is the number of \"wins\" and $\\beta$ is the number of losses.  The PDF is:\n",
    "\n",
    "$$\n",
    "PDF = \\frac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{\\beta(\\alpha, \\beta)}\\\\\n",
    "\\beta(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(2 + \\beta)}\\\\\n",
    "\\Gamma(n) = (n-1)!\n",
    "$$\n",
    "\n",
    "Unrolling that:\n",
    "\n",
    "$$\n",
    "\\frac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{\\frac{(\\alpha-1)!(\\beta-1)!}{(\\beta + 1)!}}\\\\\n",
    "= \\frac{(\\beta - 1)!\\cdot x^{\\alpha - 1}(1-x)^{\\beta - 1}}{(\\alpha-1)!(\\beta-1)!}\n",
    "$$\n",
    "\n",
    "The mean is:\n",
    "\n",
    "$$\n",
    "\\frac{\\alpha}{\\alpha + \\beta}\n",
    "$$\n",
    "\n",
    "which is just the number of wins divided by the total number of tries.  Except we're initializing $\\alpha$ and $\\beta$ to $1$.\n",
    "\n",
    "\n",
    "### BernTS($K, \\alpha, \\beta$)\n",
    "\n",
    "```\n",
    "for t = 1,2,... do\n",
    "    # sample model:\n",
    "    for k=1,...,K do\n",
    "        Sample theta_k ~ beta(a_k, b_k)\n",
    "    end for\n",
    "    \n",
    "    # select and apply action\n",
    "    x_t = argmax_k theta_k\n",
    "    Apply x_t and observe r_t\n",
    "    \n",
    "    # update distribution\n",
    "    (a_x_t, b_x_t) = (x_x_t + r_t, b_x_t + 1 - r_t)\n",
    "end for\n",
    "```\n",
    "Some things to show:\n",
    "\n",
    "- selection of actions over time\n",
    "    - see page 16 of *A Tutorial on Thompson Sampling*\n",
    "- regret over time\n",
    "    - see page 17 of *A Tutorial on Thompson Sampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "163764fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class beta(object):\n",
    "    def __init__(self,a,b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        \n",
    "\n",
    "class BernTS_bandits(object):\n",
    "    def __init__(self, probabilities , iterations=1000):\n",
    "        self.iterations=iterations\n",
    "        self.bandits = [Bandit(p=prob) for prob in probabilities]\n",
    "        self.p = probabilities.copy()\n",
    "        # Keep track of how many times each arm has been pulled\n",
    "        self.numPulls = np.zeros(len(probabilities), dtype=int)\n",
    "        \n",
    "        # Keep track of the rewards earned for each arm\n",
    "        self.rewards = np.zeros(len(probabilities))\n",
    "        \n",
    "        # value[arm] = rewards[arm]/numPulls[arm]\n",
    "        self.values = np.zeros(len(probabilities))\n",
    "        \n",
    "        self.betas = [beta(1,1) for prob in probabilities]\n",
    "        \n",
    "    def run(self):\n",
    "        result = 0\n",
    "        actions = []\n",
    "        regret = []\n",
    "        mu = 0\n",
    "        for ii in range(self.iterations):\n",
    "            bestValue = 0\n",
    "            bestAction = 0\n",
    "            for k, beta_k in enumerate(self.betas):\n",
    "                tmp_x = np.random.beta(beta_k.a, beta_k.b)\n",
    "                if tmp_x > bestValue:\n",
    "                    bestValue = tmp_x\n",
    "                    bestAction = k\n",
    "            tmp = self.bandits[bestAction].pull_arm()\n",
    "            actions.append(bestAction)\n",
    "            self.rewards[bestAction] += tmp\n",
    "            result += tmp\n",
    "            self.betas[bestAction].a += tmp\n",
    "            self.betas[bestAction].b += 1-tmp\n",
    "            self.numPulls[bestAction] += 1\n",
    "            mu += self.p[bestAction]\n",
    "            regret.append(max(self.p)*ii - mu)\n",
    "        \n",
    "        return result, actions, regret\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        #result += \"bandit\\tp\\treward\\tnumPulls\\n\"\n",
    "        total = 0\n",
    "        totalPulls = 0\n",
    "        for ii, reward in enumerate(self.rewards):\n",
    "            total += reward\n",
    "            #result += f\"{ii}\\t{self.bandits[ii].get_probability()}\\t{reward}\\t{self.numPulls[ii]}\\n\"\n",
    "            totalPulls += self.numPulls[ii]\n",
    "        result += f\"Reward {total} / Total # Pulls: {totalPulls} = {total / totalPulls}\"\n",
    "        return result\n",
    "    \n",
    "    def get_reward(self):\n",
    "        result = 0\n",
    "        for ii, reward in enumerate(self.rewards):\n",
    "            result += reward\n",
    "        return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ed600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_everything:\n",
    "    probabilities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    iterations = 1000\n",
    "    \n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    for i in range(iterations):\n",
    "        greedy = BernTS_bandits(probabilities = probabilities, iterations=100000)\n",
    "    \n",
    "        result, actions, regret = greedy.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "        \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('bernts-regret1.pdf', format='pdf')\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('Actions_bernts1.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(results)\n",
    "    ax.set_ylabel(\"$ winnings\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('bernts1.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylabel(\"# runs\")\n",
    "    ax.set_xlabel(\"$ winnings\")\n",
    "    ax.hist(results, bins='auto')\n",
    "    plt.savefig('bernts1-histo.pdf', format='pdf')\n",
    "    \n",
    "    \n",
    "    probabilities = [0.3,0.5,0.7,0.83,0.85]\n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        greedy = BernTS_bandits(probabilities = probabilities, iterations=100000)\n",
    "        result, actions, regret = greedy.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('bernts-regret2.pdf', format='pdf')\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.savefig('Actions_bernts2.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(results)\n",
    "    ax.set_ylabel(\"$ winnings\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    plt.savefig('bernts2.pdf', format='pdf')\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylabel(\"# runs\")\n",
    "    ax.set_xlabel(\"$ winnings\")\n",
    "    ax.hist(results, bins='auto')\n",
    "    plt.savefig('bernts2-histo.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e70a7",
   "metadata": {},
   "source": [
    "## problem 3\n",
    "(10 points; based on AIMA 16.3) The St. Petersburg paradox goes as follows. A fair coin is tossed repeatedly until it comes up heads. If the first heads appears on the nth toss, you win $ \\$2^n$. First, show that the expected monetary value of this game is infinite (the paradox is that no one would actually pay a huge amount to play this game). Second, consider a possible resolution of the paradox: suppose your utility for money is given by $a \\log_2 x + b$ where $x$ is the number of dollars you have. Suppose you start with 0 dollars, what is the expected utility of this game?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75471635",
   "metadata": {},
   "source": [
    "The probability of winning in $n$ turns is \n",
    "\n",
    "$$\n",
    "\\Big(\\frac{1}{2}\\Big)^n\n",
    "$$\n",
    "\n",
    "There are $(n-1)$ tails followed by 1 heads.  Each coin flip has a probability $\\frac{1}{2}$ of being tails (or heads).  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(X) & = & \\sum_{n=1}^{\\infty} \\Big(\\frac{1}{2}\\Big)^n\\cdot 2^n\\\\\n",
    "& = & \\infty\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The exponential growth of the reward, combined with a potentially infinite sequence leads to an infinite expected value.  Using a utility function that grows linearly instead of exponentially \n",
    "\n",
    "If we define $ U(x) = log_2(x)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[U(x)] & = & \\sum_{n=1}^{\\infty}\\Big(\\frac{1}{2}\\Big)^n \\cdot \\log_2(2^n)\\\\\n",
    "& = & \\sum_{n=1}^{\\infty}\\Big(\\frac{1}{2}\\Big)^n\\cdot n\\\\\n",
    "& = & \\sum_{n=1}^{\\infty}\\frac{n}{2^n}\\\\\n",
    "& = & \\frac{1}{2} + \\frac{2}{2^2} + \\frac{3}{2^3} + \\frac{4}{2^4} + \\frac{5}{2^5} + \\frac{6}{2^6} + \\cdots\\\\\n",
    "& = & \\frac{1}{2} + \\frac{1}{2^2} + \\frac{1}{2^2} + \\frac{1}{2^3} + \\frac{2}{2^3} + \\frac{1}{2^4} + \\frac{3}{2^4} + \\frac{1}{2^5} + \\frac{4}{2^5} + \\frac{1}{2^6} + \\frac{5}{2^6} + \\cdots\\\\\n",
    "& = & \\Big(\\frac{1}{2} + \\frac{1}{2^2} + \\frac{1}{2^3}  + \\frac{1}{2^4} + \\frac{1}{2^5}  + \\frac{1}{2^6} + \\cdots\\Big) + \\Big( \\frac{1}{2^2} + \\frac{2}{2^3} + \\frac{3}{2^4} + \\frac{4}{2^5} + \\frac{5}{2^6} + \\cdots\\Big )\\\\\n",
    "\\mathbb{E}[U(x)] & = & \\Big(\\frac{1}{2} + \\frac{1}{2^2} + \\frac{1}{2^3}  + \\frac{1}{2^4} + \\frac{1}{2^5}  + \\frac{1}{2^6} + \\cdots\\Big) + \\frac{1}{2}\\cdot\\Big( \\frac{1}{2} + \\frac{2}{2^2} + \\frac{3}{2^3} + \\frac{4}{2^4} + \\frac{5}{2^5} + \\cdots\\Big )\\\\\n",
    "\\mathbb{E}[U(x)] & = & \\Big(\\frac{1}{2} + \\frac{1}{2^2} + \\frac{1}{2^3}  + \\frac{1}{2^4} + \\frac{1}{2^5}  + \\frac{1}{2^6} + \\cdots\\Big) + \\frac{1}{2}\\mathbb{E}[U(x)]\\\\\n",
    "\\frac{1}{2}\\mathbb{E}[U(x)] & = & \\Big(\\frac{1}{2} + \\frac{1}{2^2} + \\frac{1}{2^3}  + \\frac{1}{2^4} + \\frac{1}{2^5}  + \\frac{1}{2^6} + \\cdots\\Big)\\\\\n",
    "\\mathbb{E}[U(x)] & = & 2\\cdot \\frac{1}{1-\\frac{1}{2}}\\\\\n",
    "& = & 4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7532d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t2.0\t0.5\t1.0\t0.5\n",
      "2\t4.0\t0.25\t2.0\t0.5\n",
      "3\t8.0\t0.125\t3.0\t0.375\n",
      "4\t16.0\t0.0625\t4.0\t0.25\n",
      "5\t32.0\t0.03125\t5.0\t0.15625\n",
      "6\t64.0\t0.015625\t6.0\t0.09375\n",
      "7\t128.0\t0.0078125\t7.0\t0.0546875\n",
      "8\t256.0\t0.00390625\t8.0\t0.03125\n",
      "9\t512.0\t0.001953125\t9.0\t0.017578125\n",
      "10\t1024.0\t0.0009765625\t10.0\t0.009765625\n",
      "11\t2048.0\t0.00048828125\t11.0\t0.00537109375\n",
      "12\t4096.0\t0.000244140625\t12.0\t0.0029296875\n",
      "13\t8192.0\t0.0001220703125\t13.0\t0.0015869140625\n",
      "14\t16384.0\t6.103515625e-05\t14.0\t0.0008544921875\n",
      "15\t32768.0\t3.0517578125e-05\t15.0\t0.000457763671875\n",
      "16\t65536.0\t1.52587890625e-05\t16.0\t0.000244140625\n",
      "17\t131072.0\t7.62939453125e-06\t17.0\t0.00012969970703125\n",
      "18\t262144.0\t3.814697265625e-06\t18.0\t6.866455078125e-05\n",
      "19\t524288.0\t1.9073486328125e-06\t19.0\t3.62396240234375e-05\n",
      "20\t1048576.0\t9.5367431640625e-07\t20.0\t1.9073486328125e-05\n",
      "21\t2097152.0\t4.76837158203125e-07\t21.0\t1.0013580322265625e-05\n",
      "22\t4194304.0\t2.384185791015625e-07\t22.0\t5.245208740234375e-06\n",
      "23\t8388608.0\t1.1920928955078125e-07\t23.0\t2.7418136596679688e-06\n",
      "24\t16777216.0\t5.960464477539063e-08\t24.0\t1.430511474609375e-06\n",
      "25\t33554432.0\t2.9802322387695312e-08\t25.0\t7.450580596923828e-07\n",
      "26\t67108864.0\t1.4901161193847656e-08\t26.0\t3.8743019104003906e-07\n",
      "27\t134217728.0\t7.450580596923828e-09\t27.0\t2.0116567611694336e-07\n",
      "28\t268435456.0\t3.725290298461914e-09\t28.0\t1.043081283569336e-07\n",
      "29\t536870912.0\t1.862645149230957e-09\t29.0\t5.4016709327697754e-08\n",
      "30\t1073741824.0\t9.313225746154785e-10\t30.0\t2.7939677238464355e-08\n",
      "31\t2147483648.0\t4.656612873077393e-10\t31.0\t1.4435499906539917e-08\n",
      "32\t4294967296.0\t2.3283064365386963e-10\t32.0\t7.450580596923828e-09\n",
      "33\t8589934592.0\t1.1641532182693481e-10\t33.0\t3.841705620288849e-09\n",
      "34\t17179869184.0\t5.820766091346741e-11\t34.0\t1.979060471057892e-09\n",
      "35\t34359738368.0\t2.9103830456733704e-11\t35.0\t1.0186340659856796e-09\n",
      "36\t68719476736.0\t1.4551915228366852e-11\t36.0\t5.238689482212067e-10\n",
      "37\t137438953472.0\t7.275957614183426e-12\t37.0\t2.6921043172478676e-10\n",
      "38\t274877906944.0\t3.637978807091713e-12\t38.0\t1.382431946694851e-10\n",
      "39\t549755813888.0\t1.8189894035458565e-12\t39.0\t7.09405867382884e-11\n",
      "40\t1099511627776.0\t9.094947017729282e-13\t40.0\t3.637978807091713e-11\n",
      "41\t2199023255552.0\t4.547473508864641e-13\t41.0\t1.864464138634503e-11\n",
      "42\t4398046511104.0\t2.2737367544323206e-13\t42.0\t9.549694368615746e-12\n",
      "43\t8796093022208.0\t1.1368683772161603e-13\t43.0\t4.888534022029489e-12\n",
      "44\t17592186044416.0\t5.684341886080802e-14\t44.0\t2.5011104298755527e-12\n",
      "45\t35184372088832.0\t2.842170943040401e-14\t45.0\t1.2789769243681803e-12\n",
      "46\t70368744177664.0\t1.4210854715202004e-14\t46.0\t6.536993168992922e-13\n",
      "47\t140737488355328.0\t7.105427357601002e-15\t47.0\t3.339550858072471e-13\n",
      "48\t281474976710656.0\t3.552713678800501e-15\t48.0\t1.7053025658242404e-13\n",
      "49\t562949953421312.0\t1.7763568394002505e-15\t49.0\t8.704148513061227e-14\n"
     ]
    }
   ],
   "source": [
    "def U(s):\n",
    "    return 1*np.log2(s)\n",
    "\n",
    "def P(s):\n",
    "    return (np.float_power(.5,s))\n",
    "\n",
    "for i in range(1,50,1):\n",
    "    cash = np.float_power(2,i)\n",
    "    print(f\"{i}\\t{cash}\\t{P(i)}\\t{U(cash)}\\t{P(i)*U(cash)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc47a1",
   "metadata": {},
   "source": [
    "## problem 4\n",
    "(10 points; based on AIMA 16.17) You are considering buying a used car, which is being offered for \\\\$1500. It is worth \\\\$2000 to you if it is in good shape, and \\\\$1300 if it is in bad shape. There is a 70\\% chance that it is in good shape. Your decision is simple: buy the car, or donâ€™t. But, as youâ€™re examining it, a mechanic you know and trust drives by and sees you looking at the car. He tells you that he has a quick test he can do: if the car is in good shape, there is an 80\\% chance that it will pass the test, and if the car is in bad shape, there is only a 35\\% chance that it will pass the test. How much would you be willing to pay the mechanic to undertake the test? Give an explanation in laymanâ€™s terms of why the value of information in this case comes out to what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2028cfdf",
   "metadata": {},
   "source": [
    "## problem 5\n",
    "(10 points; based on AIMA 17.10) Consider an undiscounted MDP with three states, (1, 2, 3) with rewards âˆ’1, âˆ’2, 0 respectively, and state 3 as a terminal state. In states 1 and 2, there are two possible actions, a and b. The transition model is as follows:\n",
    "\n",
    "- In state 1, $a$ transitions to state 2 with probability $0.8$ and stays in state 1 with probability $0.2$.\n",
    "- In state 2, $a$ transitions to state 1 with probability $0.8$ and stays in state 2 with probability $0.2$.\n",
    "- In either state 1 or state 2, $b$ transitions to state 3 with probability $0.1$ and stays in the original state with probability $0.9$.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "(a) Before doing any math, what can you say qualitatively about the optimal policy in states 1 and 2?\n",
    "\n",
    "(b) Apply policy iteration, starting with the policy that takes action b in both states, to determine the optimal policy and the values of states 1 and 2. Show each step in full.\n",
    "\n",
    "(c) Repeat part (b), but starting with the policy that takes action a in both states. What is the problem here? Does discounting help? How does the discount factor affect the optimal policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a896259",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "states = [ 0, 1, 2]\n",
    "rewards = [ -1, -2, 0]\n",
    "def transition_probabilities(s, a):\n",
    "    if (a=='a'):\n",
    "        action = 0\n",
    "    if (a=='b'):\n",
    "        action = 1\n",
    "    T=[[[0.2, 0.8, 0], [0.9, 0, 0.1]],\n",
    "       [[0.8, 0.2, 0], [0, 0.9, 0.1]],\n",
    "       [[0 ,0,1], [0,0,1]]]\n",
    "    return T[s][action]\n",
    "\n",
    "def evaluate_policy(states, T, policy, rewards, gamma=.99999):\n",
    "    P = np.zeros((len(states), len(states)))\n",
    "    for i in states:\n",
    "        tprob = T(i, policy[i])\n",
    "        for j in range(len(tprob)):\n",
    "            P[i,j] = tprob[j]\n",
    "    I = np.eye(len(states))\n",
    "    print(f\"policy: ${policy}\")\n",
    "    print(f\"${I} - ${gamma}${P}\")\n",
    "    A=np.subtract(I, gamma*P)\n",
    "    print(f\"${A}U = ${rewards}\")\n",
    "    U = np.linalg.solve(A, rewards)\n",
    "    print(f\"U = ${U}\")\n",
    "    return U\n",
    "\n",
    "def policy_iteration(states, T, rewards, startingpolicy, num_iterations, gamma=0.99999):\n",
    "    values = rewards.copy()\n",
    "    all_values = []\n",
    "    all_values.append(values.copy())\n",
    "    \n",
    "    all_policies = []\n",
    "    all_policies.append(startingpolicy.copy())\n",
    "    # Get random policy\n",
    "    policy = startingpolicy.copy()\n",
    "    \n",
    "    for ii in range(num_iterations):        \n",
    "        values = evaluate_policy(states, T, policy, rewards, gamma)\n",
    "        all_values.append(values.copy())\n",
    "        # print(f\"policy iteration - iteration {ii}\")\n",
    "        # Update the policy\n",
    "        \n",
    "        newPolicy = policy.copy()\n",
    "        for state in states:\n",
    "            maxValue = -9999999999\n",
    "            bestAction = -1\n",
    "            actions = [\"a\", \"b\"]\n",
    "            for action in actions:\n",
    "                t = T(state, action)\n",
    "                runningValue = 0.0\n",
    "                print(f\"compute value of taking action ${action}$ in state {state + 1}\\\\\\\\\")\n",
    "                for new_state, p_state in enumerate(t):\n",
    "                    print(f\"  ${state+1}\\mapsto {new_state + 1}$ case: runningValue += \\\\\\\\\")\n",
    "                    print(f\"${p_state}\\cdot({rewards[new_state]} + {gamma}\\cdot{values[new_state]})$\\\\\\\\\")\n",
    "                    print(f\"$= {p_state*(rewards[new_state] + gamma*values[new_state])}$\\\\\\\\\")\n",
    "                    runningValue += p_state*(rewards[new_state] + gamma*values[new_state])\n",
    "                print(f\"runningValue for ${state+1}$, ${action}$ is {runningValue}\\\\\\\\\")                                                                                                         \n",
    "                if runningValue > maxValue:\n",
    "                    maxValue = runningValue\n",
    "                    bestAction = action\n",
    "            newPolicy[state] = bestAction\n",
    "            print(f\"  ${bestAction}$ is the new policy for state {state + 1}\\\\\\\\\")\n",
    "            \n",
    "        # Terminate (break) if the policy does not change between steps\n",
    "\n",
    "        if np.array_equiv(policy, newPolicy):\n",
    "            break\n",
    "        policy = newPolicy\n",
    "        all_policies.append(policy.copy())\n",
    "            \n",
    "    return policy, all_values, all_policies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238823bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_everything:\n",
    "    policy = [\"b\", \"b\", \"b\"]\n",
    "    \n",
    "    newpolicy, all_values, all_policies = policy_iteration(states, transition_probabilities, rewards, policy, 10000)\n",
    "    print(newpolicy)\n",
    "    \n",
    "    print(\"All values:\")\n",
    "    print(all_values)\n",
    "    print(\"All policies:\")\n",
    "    print(all_policies)\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Policy evaluation iteration\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values)\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    policy = [\"a\", \"a\", \"a\"]\n",
    "    \n",
    "    newpolicy, all_values, all_policies = policy_iteration(states, transition_probabilities, rewards, policy, 10000)\n",
    "    print(newpolicy)\n",
    "    \n",
    "    print(\"All values:\")\n",
    "    print(all_values)\n",
    "    print(\"All policies:\")\n",
    "    print(all_policies)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Policy evaluation iteration\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values)\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    policy = [\"a\", \"b\", \"b\"]\n",
    "    \n",
    "    newpolicy, all_values, all_policies = policy_iteration(states, transition_probabilities, rewards, policy, 10000)\n",
    "    print(newpolicy)\n",
    "    \n",
    "    print(\"All values:\")\n",
    "    print(all_values)\n",
    "    print(\"All policies:\")\n",
    "    print(all_policies)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Policy evaluation iteration\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values)\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    \n",
    "    \n",
    "    policy = [\"b\", \"b\", \"b\"]\n",
    "    \n",
    "    newpolicy, all_values, all_policies = policy_iteration(states, transition_probabilities, rewards, policy, 10000, gamma=.2)\n",
    "    print(newpolicy)\n",
    "    print(\"All values:\")\n",
    "    print(all_values)\n",
    "    print(\"All policies:\")\n",
    "    print(all_policies)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Policy evaluation iteration\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values)\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    \n",
    "    policy = [\"b\", \"b\", \"b\"]\n",
    "    \n",
    "    newpolicy, all_values, all_policies = policy_iteration(states, transition_probabilities, rewards, policy, 10000, gamma=.999999999999)\n",
    "    print(newpolicy)\n",
    "    \n",
    "    print(\"All values:\")\n",
    "    print(all_values)\n",
    "    print(\"All policies:\")\n",
    "    print(all_policies)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"Policy evaluation iteration\")\n",
    "    ax.set_xlabel(\"Values of states\")\n",
    "    plt.imshow(all_values)\n",
    "    plt.colorbar()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9d658",
   "metadata": {},
   "source": [
    "### Functions to make it easier to use\n",
    "\n",
    "In hindsight, should have created these and used them myself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e77034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_using_value_iteration(p):\n",
    "    U, all_values, num_iterations = value_iteration(p)\n",
    "    pol = compute_policy(U, p)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(U)\n",
    "    ax.set_ylabel(\"U*(s) (value of being in state)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylim(0,50)\n",
    "    ax.plot(pol, 'b*')\n",
    "    ax.set_ylabel(\"action ($ to bet)\")\n",
    "    ax.set_xlabel(\"$ (the states)\")\n",
    "    \n",
    "def bandit_greedy(probabilities=[0.3,0.5,0.7,0.83,0.85], iterations=1000, trials=10):\n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    \n",
    "    for i in range(trials):\n",
    "        bandit = Greedy_bandits(probabilities = probabilities, iterations=iterations)\n",
    "    \n",
    "        result, actions, regret = bandit.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "        \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))        \n",
    "\n",
    "\n",
    "def bandit_ucb1(probabilities=[0.3,0.5,0.7,0.83,0.85], iterations=1000, trials=10):\n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    \n",
    "    for i in range(trials):\n",
    "        bandit = UCB1_bandits(probabilities = probabilities, iterations=iterations)\n",
    "    \n",
    "        result, actions, regret = bandit.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "    \n",
    "    \n",
    "    result, actions, regret = bandit.run()\n",
    "    results.append(result)\n",
    "    regretlist.append(regret)\n",
    "    actionlists.append(actions)\n",
    "        \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    plt.title(\"Actions taken at each timestep\")\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))    \n",
    "    \n",
    "def bandit_bernts(probabilities=[0.3,0.5,0.7,0.83,0.85], iterations=1000, trials=10):\n",
    "    results = []\n",
    "    actionlists = []\n",
    "    regretlist = []\n",
    "    \n",
    "    for i in range(trials):\n",
    "        bandit = BernTS_bandits(probabilities = probabilities, iterations=iterations)\n",
    "    \n",
    "        result, actions, regret = bandit.run()\n",
    "        results.append(result)\n",
    "        regretlist.append(regret)\n",
    "        actionlists.append(actions)\n",
    "    \n",
    "    \n",
    "    result, actions, regret = bandit.run()\n",
    "    results.append(result)\n",
    "    regretlist.append(regret)\n",
    "    actionlists.append(actions)\n",
    "        \n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot([sum(x)/iterations for x in zip(*regretlist)])\n",
    "    ax.set_ylabel(\"regret\")\n",
    "    ax.set_xlabel(\"iteration #\")\n",
    "    \n",
    "    plt.figure(figsize=(24, 6))\n",
    "    plt.title(\"Actions taken at each timestep\")\n",
    "    ax=plt.subplot(121)\n",
    "    ax.set_ylabel(\"trial #\")\n",
    "    ax.set_xlabel(\"Timestep\")\n",
    "    plt.imshow(actionlists, aspect='auto', cmap=mpl.colors.ListedColormap(mpl.colormaps['Paired'].colors[:len(probabilities)]))\n",
    "    cbar=plt.colorbar(label='Action', ticks=[*range(len(probabilities))])\n",
    "    cbar.ax.set_yticklabels(map(str, probabilities))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c30f5e",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "In the code block below you can try out the following functions using data of your choosing.\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "There is one function, `policy_using_value_iteration(p)` that takes a probability as an argument.\n",
    "```\n",
    "policy_using_value_iteration(.25)\n",
    "```\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "The 3 functions are called as follows:\n",
    "```\n",
    "bandit_greedy([0.3, 0.6, 0.7], 1000)\n",
    "bandit_ucb1([0.3, 0.6, 0.7], 1000)\n",
    "bandit_bernts([0.3, 0.6, 0.7], 1000)\n",
    "```\n",
    "\n",
    "There is an optional 3rd argument which is the number of trials to perform.  The default value is 10.\n",
    "\n",
    "```\n",
    "bandit_bernts([0.3, 0.6, 0.7], 1000, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47cb447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_using_value_iteration(.25)\n",
    "#bandit_greedy([0.3, 0.6, 0.7], 1000)\n",
    "#bandit_ucb1([0.3, 0.6, 0.7], 1000)\n",
    "#bandit_bernts([0.3, 0.6, 0.7], 1000)\n",
    "\n",
    "# Example that provides a vector containing probabilities, \n",
    "# the number of iterations to run and runs 100 trials:\n",
    "#\n",
    "#bandit_bernts([0.3, 0.6, 0.7], 1000, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
